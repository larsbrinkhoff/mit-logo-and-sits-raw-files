XEROX PALO ALTO RESEARCH CENTER Systems Science Laboratory
LSI Systems Area April 5, 1979



To:	distribution--LSI et.al.

From:	Dick Lyon

Subject:	Trip Report--International Conference on
	Acoustics, Speech, and Signal Processing


stored: [Ivy]<Lyon>ICASSP79.memo

I had a great trip, after initial airline snafus, and got to
see three days of Washington, cherry blossoms included, in
the rain.

As usual, the conference sessions were terrible, due largely
to the inadequate facilities.  This does not detract from
the good technical content of the proceedings, however.

The main advantage of attending this conference, as
expected, was in the personal interactions with researchers
in the various fields represented.

I felt Xerox was very under-represented (I was the only
Xerox person there, as far as I know), compared to other
companies in the fields of imaging, facimile, dictation,
data communication, office products, etc.

Here is an interesting quote from Victor Zue's (MIT) paper
entitled "Experiments on Spectrogram Reading": ". . . our
research demonstrates that the acoustic signal is the
primary information bearer.  The results of our
spectrogram-reading experiment suggest that there exists a
great deal more phonetic information in the speech signal
than was previously believed, and that such information is
often explicit and can be captured by rules.  Therefore,
efforts directed at improving the 'front-end' of current
speech understanding systems are probably worthwhile."  Of
course, that's not news to me.

There is an emerging group of people working on speech
analysis from the point of view of accurate modeling of the
human hearing process, including Campbell Searle (Queens
University, Kingston, Ontario; formerly of MIT), his
ex-student David Cuddy (Bell Northern Research, Ottawa), his
student Barry P. Kimberly (in transition from Queens to
BNR), and Jont B. Allen (Bell Labs, Murray Hill).  Other
persons working on speech processing based on constant-Q
spectral analysis (approximately based on hearing models)
are James M. Kates (Signatron, Lexington MA) and James E.
Youngberg (CS dept. Utah and Soundstream, Inc., Salt Lake
City).  Campbell Searle is working on a paper "A Model of
Mechanical to Neural Transduction in the Cochlea", which is
similar to my own efforts in my paper "A Signal Processing
Model of Hearing".  His efforts are directed at a
comprehensive model which explains various physiological and
psychophysical experimental results, but Jont Allen says the
model does not hold water in the face of some other results,
and that mechanical cochlear nonlinearities and a second
filter are needed.  I disagree with both of them, and feel
that my own model can be made to fit fairly well.  David
Cuddy recently had funding approved for development of a
filterbank speech analysis system, as an attempt to provide
an objective voice quality measurement system for
qualification of encoding and transmission schemes; he is in
the human factors department at BNR, which is charged with
evaluating the subjective suitability of various schemes.

I spoke with Xavier Rodet, who asked if I knew Mark Kahrs,
who had worked with his group at IRCAM in France.  I told
him we were considering him for a summer position, and he
highly recommended him.

George White surprised me by telling me that he is leaving
ITT to return to the Bay area; he will have a consulting
partnership with Ben Warren, formerly of Intel, which will
be located in Palo Alto Square.

I spent considerable time talking with Bob Broderson of
Berkeley (while touring the air and space museum, etc.)
about what he is doing there in the way of speech processing
IC's.  The answer is nothing particularly exciting, but some
interesting hybrid analog/digital hacks.  He claims one of
their big advantages is fast turnaround (one month from
start of layout to chips) which they achieve by cutting Ruby
and doing all their own processing (each student does his
own!).  He admits that this explains why they typically
leave off the digital parts of their test chips, since
nobody has the patience to cut that much Ruby.  They are
currently getting digitizers and pattern generation
equipment.

Other interesting people who were there include the
following: John Treichler (ArgoSystems, Sunnyvale) is
working on high speed digital signal processing for military
communications, specifically FM demodulation.  Sally Wood
(Telesensory Systems Inc., Palo Alto) is working on the
reading machine for the blind, and has been complaining
since she started that omni-font character recognition can't
be done with the Optacon camera.  Jim and Janet Baker have
been hired away from IBM's continuous speech recognition
project to work for Dialog (Belmont, MA, an Exxon company).
Raj Reddy is willing to bet that CMU will have an automatic
dictation machine before anyone else; he didn't get much
argument.  Larry Rabiner (Bell Labs, Murray Hill) is willing
to bet that the Bell System will not have such a capability
until at least a dozen small companies have it.  John Burg
and Charlie Davis of Time and Space Processing, makers of an
LPC vocoder, want to learn more about custom LSI for
eventual integration of their product.  Meanwhile, Motorola
just received a big government contract to do just that.
Sid Maitra has just gone to work for Systems Control, Inc.,
our neighbor across the street, to work with Bruce Loweree
(of Harpy fame) on speech recognition.

c:	A. Bell
	L. Conway
	D. Fairbairn
	M. Newell
	L. Stewart
	B. Sutherland
	D. Swinehart
	W. Wilner

